# Advanced Code Artifact Extraction and Packaging System
"""
Intelligent artifact processing pipeline that extracts generated code files
from AI agent outputs and packages them into structured archives.
"""

import asyncio
import re
import io
import zipfile
from typing import List, Dict, Any
from intelligence.backend_architect import BackendArchitectureGenerator
from intelligence.frontend_designer import FrontendDesignGenerator

class ArtifactProcessor:
    """
    Sophisticated code artifact extraction and packaging engine that processes
    AI-generated content and creates organized file structures.
    """
    
    # Enhanced pattern matching for various code formats
    FILE_EXTRACTION_PATTERN = re.compile(
        r"### (?:File|Component|Module):\s*([^\n]+)\n```(?:typescript|html|scss|python|javascript|css)?\n(.*?)```",
        re.DOTALL | re.MULTILINE | re.IGNORECASE
    )
    
    # File content cleaning patterns
    CLEANUP_PATTERNS = [
        (r'# Code Generated by Sidekick is for learning and experimentation purposes only.\n?', ''),
        (r'# Auto-generated content - modify with caution\n?', ''),
        (r'# Generated by AI Assistant\n?', ''),
    ]
    
    @classmethod
    def extract_generated_artifacts(cls, ai_output_content: str) -> List[Dict[str, str]]:
        """
        Advanced artifact extraction that identifies and processes code files
        from AI agent responses with intelligent content cleaning.
        
        Args:
            ai_output_content: Raw output from AI code generation agents
            
        Returns:
            List of dictionaries containing file paths and cleaned code content
        """
        extracted_artifacts = []
        
        # Find all code blocks with associated file information
        file_matches = cls.FILE_EXTRACTION_PATTERN.finditer(ai_output_content)
        
        for match in file_matches:
            file_path = match.group(1).strip()
            raw_code_content = match.group(2).strip()
            
            # Apply content cleaning and normalization
            cleaned_content = cls._clean_generated_content(raw_code_content)
            
            # Validate and structure the artifact
            if cls._validate_artifact(file_path, cleaned_content):
                extracted_artifacts.append({
                    "file_path": cls._normalize_file_path(file_path),
                    "content": cleaned_content,
                    "content_type": cls._determine_content_type(file_path)
                })
        
        return extracted_artifacts
    
    @classmethod
    def create_compressed_archive(cls, artifact_collection: List[Dict[str, str]]) -> io.BytesIO:
        """
        Create a compressed ZIP archive containing all generated artifacts
        with proper directory structure and metadata.
        
        Args:
            artifact_collection: List of artifact dictionaries with file data
            
        Returns:
            BytesIO object containing the compressed archive
        """
        archive_buffer = io.BytesIO()
        
        with zipfile.ZipFile(archive_buffer, "w", zipfile.ZIP_DEFLATED) as zip_archive:
            for artifact in artifact_collection:
                file_path = artifact["file_path"]
                file_content = artifact["content"]
                
                # Add file to archive with proper path structure
                zip_archive.writestr(file_path, file_content)
                
            # Add metadata file for archive information
            metadata_content = cls._generate_archive_metadata(artifact_collection)
            zip_archive.writestr("_metadata.json", metadata_content)
        
        archive_buffer.seek(0)
        return archive_buffer
    
    @classmethod
    def _clean_generated_content(cls, raw_content: str) -> str:
        """
        Apply comprehensive content cleaning to remove AI-generated boilerplate
        and normalize the code formatting.
        """
        cleaned_content = raw_content
        
        # Apply all cleanup patterns
        for pattern, replacement in cls.CLEANUP_PATTERNS:
            cleaned_content = re.sub(pattern, replacement, cleaned_content)
        
        # Normalize line endings and remove excessive whitespace
        cleaned_content = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned_content)
        cleaned_content = cleaned_content.strip()
        
        return cleaned_content
    
    @classmethod
    def _validate_artifact(cls, file_path: str, content: str) -> bool:
        """
        Validate that the extracted artifact contains meaningful content
        and has a reasonable file path structure.
        """
        # Check for minimum content requirements
        if not content or len(content.strip()) < 10:
            return False
        
        # Validate file path format
        if not file_path or len(file_path.strip()) < 3:
            return False
        
        # Ensure file has a proper extension
        if '.' not in file_path:
            return False
        
        return True
    
    @classmethod
    def _normalize_file_path(cls, raw_path: str) -> str:
        """
        Normalize file paths to ensure consistent directory structure
        and remove any problematic characters.
        """
        # Remove any leading/trailing whitespace and quotes
        normalized = raw_path.strip().strip('"\'')
        
        # Ensure forward slashes for cross-platform compatibility
        normalized = normalized.replace('\\', '/')
        
        # Remove any leading slashes to prevent absolute path issues
        normalized = normalized.lstrip('/')
        
        return normalized
    
    @classmethod
    def _determine_content_type(cls, file_path: str) -> str:
        """
        Determine the content type based on file extension for better organization.
        """
        extension_map = {
            '.py': 'python',
            '.ts': 'typescript',
            '.js': 'javascript',
            '.html': 'html',
            '.css': 'css',
            '.scss': 'scss',
            '.json': 'json',
            '.md': 'markdown',
            '.txt': 'text'
        }
        
        file_extension = '.' + file_path.split('.')[-1].lower()
        return extension_map.get(file_extension, 'unknown')
    
    @classmethod
    def _generate_archive_metadata(cls, artifacts: List[Dict[str, str]]) -> str:
        """
        Generate comprehensive metadata about the archive contents for documentation.
        """
        import json
        from datetime import datetime
        
        metadata = {
            "archive_info": {
                "generated_at": datetime.now().isoformat(),
                "total_files": len(artifacts),
                "generator": "Advanced Code Generation System v2.0"
            },
            "file_inventory": [
                {
                    "path": artifact["file_path"],
                    "type": artifact["content_type"],
                    "size_bytes": len(artifact["content"])
                }
                for artifact in artifacts
            ],
            "content_summary": {
                content_type: len([a for a in artifacts if a["content_type"] == content_type])
                for content_type in set(artifact["content_type"] for artifact in artifacts)
            }
        }
        
        return json.dumps(metadata, indent=2)

# Maintain backward compatibility with original function names
def extract_files_from_agent_output(agent_output: str) -> List[Dict[str, str]]:
    """Legacy compatibility wrapper for artifact extraction."""
    artifacts = ArtifactProcessor.extract_generated_artifacts(agent_output)
    return [{"path": a["file_path"], "code": a["content"]} for a in artifacts]

def create_zip_in_memory(files: List[Dict[str, str]]) -> io.BytesIO:
    """Legacy compatibility wrapper for archive creation."""
    # Convert legacy format to new format
    artifacts = [{"file_path": f["path"], "content": f["code"], "content_type": "unknown"} for f in files]
    return ArtifactProcessor.create_compressed_archive(artifacts)

# Main function for backwards compatibility
async def main():
    """Example usage demonstration with backend code generation."""
    backend_generator = BackendArchitectureGenerator()
    sample_srd = """[SAMPLE SRD DOCUMENT]"""
    task_description = f"Generate comprehensive backend architecture based on SRD: {sample_srd}"

    # Collect AI agent outputs
    all_generated_content = ""
    async for message in backend_generator.run_stream(task=task_description):
        if hasattr(message, "content"):
            all_generated_content += str(message.content) + "\n"

    # Process and package artifacts
    extracted_artifacts = ArtifactProcessor.extract_generated_artifacts(all_generated_content)
    compressed_archive = ArtifactProcessor.create_compressed_archive(extracted_artifacts)

    # Example: Save archive for testing
    with open("generated_architecture.zip", "wb") as output_file:
        output_file.write(compressed_archive.read())

if __name__ == "__main__":
    asyncio.run(main())
